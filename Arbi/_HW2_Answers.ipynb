{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HW2 - Detecting Type 1 Diabetes***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1**\n",
    "\n",
    "For our opinion, when it comes to healthcare ML algorithms, Accuracy is more important than performance.\n",
    "Because we're speaking on human lives, it is better to be more precise than, for example, when estimating house value.\n",
    "\n",
    "האם התכוונו כאן לביצועיות של המודל? כלומר למהירות בה הוא מגיע לתוצאות?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2**\n",
    "\n",
    "Too many features is often a bad thing. It may lead to Overfitting, meaning that the fitting of your parameters is too tightly to the training data. This results in model discovering random noise in the finite training set instead of the wider relationship between the features and the output variable. Consequently, the model will often perform very well on the training data but perform quite poorly on the test data.\n",
    "In conclusion, overfitting may be done due to choosing all features, especially whenn taking irrelevant feature like income.\n",
    "\n",
    "On the other hand, choosing only 2 features for such complex problem may cause under-fitting. meaning that both training accuracy and testing accuracy will be poor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import distutils\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from Ext_Functions import str_to_bool_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1 - Data loading and preprocessing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('HW2_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer all data to numeric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ext_Functions import nan2num\n",
    "\n",
    "df = df_orig\n",
    "for column in df.columns:\n",
    "    df[column] = str_to_bool_series(df_orig[column])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean NaN's:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1d_clean = nan2num(df)\n",
    "t1d_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale Age column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "t1d_clean[['Age']] = min_max_scaler.fit_transform(t1d_clean[['Age']])\n",
    "t1d_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Q2 -  Split the data into Test Train 20%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diagnosis = t1d_clean['Diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(t1d_clean, np.ravel(diagnosis), test_size=0.2,\n",
    "                                                        random_state=0, stratify=np.ravel(diagnosis))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q3 - Visualization and exploration of the data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Ext_Functions import print_dist_features\n",
    "print_dist_features(t1d_clean, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3 Section a**\n",
    "\n",
    "Imbalance between train and test set could cause the learning algorithm to fail in classification of the test set.\n",
    "Balance can be reached with stratification or normalization. in our case, due to the booleanic nature of the data, its better to do stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship between feature and label:\n",
    "\n",
    "These plots shows the relationsship between each feature and label. The age feature is shown in zoom-in plot because it has many details and was not clear in a small plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sbn\n",
    "import matplotlib.ticker as ticker\n",
    "from Ext_Functions import feature_corr\n",
    "\n",
    "feature_corr(t1d_clean)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1,figsize=(15, 10) )\n",
    "\n",
    "\n",
    "# Plot Age seperately:\n",
    "title_text = 'Number of patients as a function of age - Zoom in'\n",
    "feat_lab = sbn.countplot(ax = axes, x='Age', hue = 'Diagnosis', data = t1d_clean)\n",
    "feat_lab.xaxis.set_major_locator(ticker.LinearLocator(10))\n",
    "feat_lab.set_title(title_text, fontsize=15)\n",
    "axes.tick_params(axis='both', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['Diagnosis'])\n",
    "X_test = X_test.drop(columns=['Diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q4 - One Hot Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have already done this step - manually - in the data pre-processing stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ohv = X_train\n",
    "X_test_ohv = X_test\n",
    "y_train_ohv = y_train\n",
    "y_test_ohv = y_test\n",
    "\n",
    "(X_train_ohv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ext_Functions import cv_kfold\n",
    "from Ext_Functions import pred_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Test and Fit the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C = np.array([0.01, 0.1, 1, 5, 10, 100]) \n",
    "K = 5  \n",
    "penalty = ['l1', 'l2']\n",
    "val_dict = cv_kfold(X_train_ohv, y_train_ohv, C=C, penalty=penalty, K=K)\n",
    "print(pd.DataFrame(val_dict))\n",
    "\n",
    "# Insert best parameters to get the best model:\n",
    "\n",
    "c = 5\n",
    "p = 'l1'\n",
    "lr_best_model = LogisticRegression(solver='saga', multi_class='ovr', penalty=p, C=c, max_iter=10000, random_state=10)\n",
    "y_pred_best, _ = pred_log(lr_best_model, X_train_ohv, y_train_ohv, X_test_ohv)\n",
    "y_pred_p_best, _ = pred_log(lr_best_model, X_train_ohv, y_train_ohv, X_test_ohv, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_splits = K\n",
    "skfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "svc_model = SVC(probability=True)\n",
    "C = np.array([1, 100, 1000])\n",
    "pipeline = Pipeline(steps=[('scale', StandardScaler()), ('svm', svc_model)])\n",
    "svm_nonlinear = GridSearchCV(estimator=pipeline, param_grid={'svm__C': C, 'svm__kernel': ['rbf', 'poly'],\n",
    "                        'svm__gamma': ['auto', 'scale']}, scoring=['roc_auc'],\n",
    "                        cv=skfold, refit='roc_auc', verbose=0, return_train_score=True)\n",
    "svm_nonlinear.fit(X_train_ohv, y_train_ohv)\n",
    "\n",
    "best_svm_nonlin = svm_nonlinear.best_estimator_\n",
    "print('Parameters to get best svm are:', svm_nonlinear.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "disp_lr = plot_confusion_matrix(lr_best_model, X_test_ohv, y_test_ohv,\n",
    "                                 display_labels=['Negative', 'Positive'],\n",
    "                                 cmap=plt.cm.Blues)\n",
    "disp_lr.ax_.set_title(\"Confusion Matrix - Logistic Regression\")\n",
    "\n",
    "disp_svm = plot_confusion_matrix(best_svm_nonlin, X_test_ohv, y_test_ohv,\n",
    "                                 display_labels=['Negative', 'Positive'],\n",
    "                                 cmap=plt.cm.Blues)\n",
    "disp_svm.ax_.set_title(\"Confusion Matrix - Non-linear SVM\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q5 part b\n",
    "\n",
    "Plot statistics for all algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from Ext_Functions import parameter_evaluation\n",
    "# lr = Logistic Regression\n",
    "# svm = Support Vector Machine\n",
    "\n",
    "\n",
    "model_lr = lr_best_model\n",
    "model_svm = best_svm_nonlin\n",
    "y_pred_lr = model_lr.decision_function(X_test_ohv)\n",
    "lr_fpr, lr_tpr, threshold = roc_curve(y_test_ohv, y_pred_lr)\n",
    "y_pred_lr_train = (model_lr.predict(X_train_ohv)).astype(\"int32\")\n",
    "y_pred_lr_test = (model_lr.predict(X_test_ohv)).astype(\"int32\")\n",
    "auc_lr = metrics.roc_auc_score(y_test,y_pred_lr_test)\n",
    "\n",
    "\n",
    "y_pred_svm = model_svm.decision_function(X_test_ohv)\n",
    "svm_fpr, svm_tpr, threshold = roc_curve(y_test_ohv, y_pred_svm)\n",
    "y_pred_svm_train = (model_svm.predict(X_train_ohv)).astype(\"int32\")\n",
    "y_pred_svm_test = (model_svm.predict(X_test_ohv)).astype(\"int32\")\n",
    "auc_svm = metrics.roc_auc_score(y_test,y_pred_svm_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5,5), dpi = 100)\n",
    "plt.plot(svm_fpr, svm_tpr, linestyle = '-', label = 'SVM (auc=%0.3f)' %auc_svm)\n",
    "plt.plot(lr_fpr, lr_tpr,marker='.' ,label = 'Logistic Reg (auc=%0.3f)' %auc_lr)\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_lr_test,y_pred_lr_train)\n",
    "\n",
    "print()\n",
    "print('SVM Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_svm_test,y_pred_svm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Q6 - Feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train,y_train)\n",
    "# get importance\n",
    "importance = model_rf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    " \tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "labels = ['Age','Gender_M', 'Increased Urination_T', 'Increased Thirst_T',\n",
    "       'Sudden Weight Loss_T', 'Weakness_T',\n",
    "        'Increased Hunger_T','Genital Thrush_T', 'Visual Blurring_T',  \n",
    "        'Itching_T', 'Irritability_T','Delayed Healing_T',\n",
    "        'Partial Paresis_T', 'Muscle Stiffness_T', 'Hair Loss_T',\n",
    "        'Obesity_T','Family History_T']\n",
    "t= np.arange(0,17)\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.xticks(t,labels, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 2 most important features are increased urination and increased thirst.\n",
    "\n",
    "This matches up with the feature - label correlation from Q3. Looking at the graphs, we can see that these features are the most meaningful ones since they have the largest number of diagnosed patients. Also, among the patients who do have increased urination\\thirst, the number of the healthy ones is the lowest in comparison to the diagnsed ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q7 - Data Separability Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "scaler = StandardScaler()\n",
    "X_train_copy = scaler.fit_transform(X_train_ohv)\n",
    "X_test_copy = scaler.transform(X_test_ohv)\n",
    "X_train_pca = pca.fit_transform(X_train_copy)\n",
    "X_test_pca = pca.transform(X_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plt_2d_pca(X_pca,y):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='b')\n",
    "    ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='r')\n",
    "    ax.legend(('Negative','Positive'))\n",
    "    ax.plot([0], [0], \"ko\")\n",
    "    ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.set_xlabel('$U_1$')\n",
    "    ax.set_ylabel('$U_2$')\n",
    "    ax.set_title('2D PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt_2d_pca(X_test_pca,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section b\n",
    "\n",
    "As can be seen above, the data is not linearly seperable. It is possible to use a linear classifier, however the boundary condition may be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import hinge_loss\n",
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the perfect parameters for the new-2D-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Linear Regression:\n",
    "\n",
    "X_train_pca = pd.DataFrame(X_train_pca)\n",
    "X_test_pca = pd.DataFrame(X_test_pca)\n",
    "\n",
    "C = np.array([0.01, 0.1, 1, 5, 10, 100]) \n",
    "K = 5  \n",
    "penalty = ['l1', 'l2']\n",
    "val_dict_pca = cv_kfold(X_train_pca, y_train_ohv, C=C, penalty=penalty, K=K)\n",
    "print(pd.DataFrame(val_dict_pca))\n",
    "\n",
    "# Insert best parameters to get the best model (PCA):\n",
    "\n",
    "c = 1\n",
    "p = 'l2'\n",
    "lr_best_model_pca = LogisticRegression(solver='saga', multi_class='ovr', penalty=p, C=c, max_iter=10000, random_state=10)\n",
    "y_pred_best_pca, _ = pred_log(lr_best_model_pca, X_train_pca, y_train_ohv, X_test_pca)\n",
    "y_pred_prob_best_pca, _ = pred_log(lr_best_model_pca, X_train_pca, y_train_ohv, X_test_pca, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = K\n",
    "skfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "svc_model = SVC(probability=True)\n",
    "C = np.array([1, 100, 1000])\n",
    "pipeline = Pipeline(steps=[('scale', StandardScaler()), ('svm', svc_model)])\n",
    "svm_nonlinear_pca = GridSearchCV(estimator=pipeline, param_grid={'svm__C': C, 'svm__kernel': ['rbf', 'poly'],\n",
    "                        'svm__gamma': ['auto', 'scale']}, scoring=['roc_auc'],\n",
    "                        cv=skfold, refit='roc_auc', verbose=0, return_train_score=True)\n",
    "svm_nonlinear_pca.fit(X_train_pca, y_train_ohv)\n",
    "\n",
    "best_svm_nonlin_pca = svm_nonlinear_pca.best_estimator_\n",
    "print('Parameters to get best svm are:', svm_nonlinear_pca.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predict + fit for LR and SVM (PCA):\n",
    "\n",
    "lr_best_model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_test_pca = lr_best_model_pca.predict(X_test_pca)\n",
    "\n",
    "model_lr_pca = lr_best_model_pca\n",
    "model_svm_pca = best_svm_nonlin_pca\n",
    "\n",
    "y_pred_lr_pca = model_lr_pca.decision_function(X_test_pca)\n",
    "lr_fpr_pca, lr_tpr_pca, threshold_pca = roc_curve(y_test_ohv, y_pred_lr_pca)\n",
    "y_pred_lr_train_pca = (model_lr_pca.predict(X_train_pca)).astype(\"int32\")\n",
    "y_pred_lr_test_pca = (model_lr_pca.predict(X_test_pca)).astype(\"int32\")\n",
    "auc_lr_pca = metrics.roc_auc_score(y_test,y_pred_lr_test_pca)\n",
    "\n",
    "\n",
    "y_pred_svm_pca = best_svm_nonlin_pca.decision_function(X_test_pca)\n",
    "svm_fpr_pca, svm_tpr_pca, threshold_pca = roc_curve(y_test_ohv, y_pred_svm_pca)\n",
    "y_pred_svm_train_pca = (best_svm_nonlin_pca.predict(X_train_pca)).astype(\"int32\")\n",
    "y_pred_svm_test_pca = (best_svm_nonlin_pca.predict(X_test_pca)).astype(\"int32\")\n",
    "auc_svm_pca = metrics.roc_auc_score(y_test,y_pred_svm_test_pca)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5,5), dpi = 100)\n",
    "plt.plot(svm_fpr_pca, svm_tpr_pca, linestyle = '-', label = 'SVM - 2D (auc=%0.3f)' %auc_svm_pca)\n",
    "plt.plot(lr_fpr_pca, lr_tpr_pca,marker='.' ,label = 'Logistic Reg - 2D(auc=%0.3f)' %auc_lr_pca)\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_lr_test_pca,y_pred_lr_train_pca)\n",
    "\n",
    "print()\n",
    "print('SVM Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_svm_test_pca,y_pred_svm_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat_name =['Increased Urination','Increased Thirst']\n",
    "X_train_2f = X_train[feat_name]\n",
    "X_test_2f =  X_test[feat_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - finding the best parameters for the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Linear Regression:\n",
    "\n",
    "X_train_2f = pd.DataFrame(X_train_2f)\n",
    "X_test_2f = pd.DataFrame(X_test_2f)\n",
    "\n",
    "C = np.array([0.01, 0.1, 1, 5, 10, 100]) \n",
    "K = 5  \n",
    "penalty = ['l1', 'l2']\n",
    "val_dict_2f = cv_kfold(X_train_2f, y_train_ohv, C=C, penalty=penalty, K=K)\n",
    "print(pd.DataFrame(val_dict_2f))\n",
    "\n",
    "# Insert best parameters to get the best model (PCA):\n",
    "\n",
    "c = 0.1\n",
    "p = 'l2'\n",
    "lr_best_model_2f = LogisticRegression(solver='saga', multi_class='ovr', penalty=p, C=c, max_iter=10000, random_state=10)\n",
    "y_pred_best_2f, _ = pred_log(lr_best_model_2f, X_train_2f, y_train_ohv, X_test_2f)\n",
    "y_pred_prob_best_2f, _ = pred_log(lr_best_model_2f, X_train_2f, y_train_ohv, X_test_pca, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = K\n",
    "skfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "svc_model_2f = SVC(probability=True)\n",
    "C = np.array([1, 100, 1000])\n",
    "pipeline = Pipeline(steps=[('scale', StandardScaler()), ('svm', svc_model_2f)])\n",
    "svm_nonlinear_2f = GridSearchCV(estimator=pipeline, param_grid={'svm__C': C, 'svm__kernel': ['rbf', 'poly'],\n",
    "                        'svm__gamma': ['auto', 'scale']}, scoring=['roc_auc'],\n",
    "                        cv=skfold, refit='roc_auc', verbose=0, return_train_score=True)\n",
    "svm_nonlinear_2f.fit(X_train_2f, y_train_ohv)\n",
    "\n",
    "best_svm_nonlin_2f = svm_nonlinear_2f.best_estimator_\n",
    "print('Parameters to get best svm are:', svm_nonlinear_2f.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Predict + fit for LR and SVM (PCA):\n",
    "\n",
    "lr_best_model_2f.fit(X_train_2f, y_train)\n",
    "y_pred_test_2f = lr_best_model_2f.predict(X_test_2f)\n",
    "\n",
    "model_lr_2f = lr_best_model_2f\n",
    "model_svm_2f = best_svm_nonlin_2f\n",
    "\n",
    "y_pred_lr_2f = lr_best_model_2f.decision_function(X_test_2f)\n",
    "lr_fpr_2f, lr_tpr_2f, threshold_2f = roc_curve(y_test_ohv, y_pred_lr_2f)\n",
    "y_pred_lr_train_2f = (lr_best_model_2f.predict(X_train_2f)).astype(\"int32\")\n",
    "y_pred_lr_test_2f = (model_lr_2f.predict(X_test_2f)).astype(\"int32\")\n",
    "auc_lr_2f = metrics.roc_auc_score(y_test,y_pred_lr_test_2f)\n",
    "\n",
    "\n",
    "y_pred_svm_2f = svm_nonlinear_2f.decision_function(X_test_2f)\n",
    "svm_fpr_2f, svm_tpr_2f, threshold_2f = roc_curve(y_test_ohv, y_pred_svm_2f)\n",
    "y_pred_svm_train_2f = (svm_nonlinear_2f.predict(X_train_2f)).astype(\"int32\")\n",
    "y_pred_svm_test_2f = (svm_nonlinear_2f.predict(X_test_2f)).astype(\"int32\")\n",
    "auc_svm_2f = metrics.roc_auc_score(y_test,y_pred_svm_test_2f)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5,5), dpi = 100)\n",
    "plt.plot(svm_fpr_2f, svm_tpr_2f, linestyle = '-', label = 'SVM - 2D (auc=%0.3f)' %auc_svm_2f)\n",
    "plt.plot(lr_fpr_2f, lr_tpr_2f,marker='.' ,label = 'Logistic Reg - 2D(auc=%0.3f)' %auc_lr_2f)\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_lr_test_2f,y_pred_lr_train_2f)\n",
    "\n",
    "print()\n",
    "print('SVM Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_svm_test_2f,y_pred_svm_train_2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "scaler = StandardScaler()\n",
    "X_train_copy = scaler.fit_transform(X_train_ohv)\n",
    "X_test_copy = scaler.transform(X_test_ohv)\n",
    "X_train_pca = pca.fit_transform(X_train_copy)\n",
    "X_test_pca = pca.transform(X_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_2d_pca(X_pca,y):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    ax.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='b')\n",
    "    ax.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='r')\n",
    "    ax.legend(('Negative','Positive'))\n",
    "    ax.plot([0], [0], \"ko\")\n",
    "    ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "    ax.set_xlabel('$U_1$')\n",
    "    ax.set_ylabel('$U_2$')\n",
    "    ax.set_title('2D PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_2d_pca(X_test_pca,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section b\n",
    "\n",
    "As can be seen above, the data is not linearly seperable. It is possible to use a linear classifier, however the boundary condition may be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import hinge_loss\n",
    "calc_TN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]\n",
    "calc_FP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]\n",
    "calc_FN = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]\n",
    "calc_TP = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the perfect parameters for the new-2D-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression:\n",
    "\n",
    "X_train_pca = pd.DataFrame(X_train_pca)\n",
    "X_test_pca = pd.DataFrame(X_test_pca)\n",
    "\n",
    "C = np.array([0.01, 0.1, 1, 5, 10, 100]) \n",
    "K = 5  \n",
    "penalty = ['l1', 'l2']\n",
    "val_dict_pca = cv_kfold(X_train_pca, y_train_ohv, C=C, penalty=penalty, K=K)\n",
    "print(pd.DataFrame(val_dict_pca))\n",
    "\n",
    "# Insert best parameters to get the best model (PCA):\n",
    "\n",
    "c = 1\n",
    "p = 'l2'\n",
    "lr_best_model_pca = LogisticRegression(solver='saga', multi_class='ovr', penalty=p, C=c, max_iter=10000, random_state=10)\n",
    "y_pred_best_pca, _ = pred_log(lr_best_model_pca, X_train_pca, y_train_ohv, X_test_pca)\n",
    "y_pred_prob_best_pca, _ = pred_log(lr_best_model_pca, X_train_pca, y_train_ohv, X_test_pca, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = K\n",
    "skfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "svc_model = SVC(probability=True)\n",
    "C = np.array([1, 100, 1000])\n",
    "pipeline = Pipeline(steps=[('scale', StandardScaler()), ('svm', svc_model)])\n",
    "svm_nonlinear_pca = GridSearchCV(estimator=pipeline, param_grid={'svm__C': C, 'svm__kernel': ['rbf', 'poly'],\n",
    "                        'svm__gamma': ['auto', 'scale']}, scoring=['roc_auc'],\n",
    "                        cv=skfold, refit='roc_auc', verbose=0, return_train_score=True)\n",
    "svm_nonlinear_pca.fit(X_train_pca, y_train_ohv)\n",
    "\n",
    "best_svm_nonlin_pca = svm_nonlinear_pca.best_estimator_\n",
    "print('Parameters to get best svm are:', svm_nonlinear_pca.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict + fit for LR and SVM (PCA):\n",
    "\n",
    "lr_best_model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_test_pca = lr_best_model_pca.predict(X_test_pca)\n",
    "\n",
    "model_lr_pca = lr_best_model_pca\n",
    "model_svm_pca = best_svm_nonlin_pca\n",
    "\n",
    "y_pred_lr_pca = model_lr_pca.decision_function(X_test_pca)\n",
    "lr_fpr_pca, lr_tpr_pca, threshold_pca = roc_curve(y_test_ohv, y_pred_lr_pca)\n",
    "y_pred_lr_train_pca = (model_lr_pca.predict(X_train_pca)).astype(\"int32\")\n",
    "y_pred_lr_test_pca = (model_lr_pca.predict(X_test_pca)).astype(\"int32\")\n",
    "auc_lr_pca = metrics.roc_auc_score(y_test,y_pred_lr_test_pca)\n",
    "\n",
    "\n",
    "y_pred_svm_pca = best_svm_nonlin_pca.decision_function(X_test_pca)\n",
    "svm_fpr_pca, svm_tpr_pca, threshold_pca = roc_curve(y_test_ohv, y_pred_svm_pca)\n",
    "y_pred_svm_train_pca = (best_svm_nonlin_pca.predict(X_train_pca)).astype(\"int32\")\n",
    "y_pred_svm_test_pca = (best_svm_nonlin_pca.predict(X_test_pca)).astype(\"int32\")\n",
    "auc_svm_pca = metrics.roc_auc_score(y_test,y_pred_svm_test_pca)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5,5), dpi = 100)\n",
    "plt.plot(svm_fpr_pca, svm_tpr_pca, linestyle = '-', label = 'SVM - 2D (auc=%0.3f)' %auc_svm_pca)\n",
    "plt.plot(lr_fpr_pca, lr_tpr_pca,marker='.' ,label = 'Logistic Reg - 2D(auc=%0.3f)' %auc_lr_pca)\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_lr_test_pca,y_pred_lr_train_pca)\n",
    "\n",
    "print()\n",
    "print('SVM Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_svm_test_pca,y_pred_svm_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name =['Increased Urination','Increased Thirst']\n",
    "X_train_2f = X_train[feat_name]\n",
    "X_test_2f =  X_test[feat_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - finding the best parameters for the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression:\n",
    "\n",
    "X_train_2f = pd.DataFrame(X_train_2f)\n",
    "X_test_2f = pd.DataFrame(X_test_2f)\n",
    "\n",
    "C = np.array([0.01, 0.1, 1, 5, 10, 100]) \n",
    "K = 5  \n",
    "penalty = ['l1', 'l2']\n",
    "val_dict_2f = cv_kfold(X_train_2f, y_train_ohv, C=C, penalty=penalty, K=K)\n",
    "print(pd.DataFrame(val_dict_2f))\n",
    "\n",
    "# Insert best parameters to get the best model (PCA):\n",
    "\n",
    "c = 0.1\n",
    "p = 'l2'\n",
    "lr_best_model_2f = LogisticRegression(solver='saga', multi_class='ovr', penalty=p, C=c, max_iter=10000, random_state=10)\n",
    "y_pred_best_2f, _ = pred_log(lr_best_model_2f, X_train_2f, y_train_ohv, X_test_2f)\n",
    "y_pred_prob_best_2f, _ = pred_log(lr_best_model_2f, X_train_2f, y_train_ohv, X_test_pca, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = K\n",
    "skfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\n",
    "svc_model_2f = SVC(probability=True)\n",
    "C = np.array([1, 100, 1000])\n",
    "pipeline = Pipeline(steps=[('scale', StandardScaler()), ('svm', svc_model_2f)])\n",
    "svm_nonlinear_2f = GridSearchCV(estimator=pipeline, param_grid={'svm__C': C, 'svm__kernel': ['rbf', 'poly'],\n",
    "                        'svm__gamma': ['auto', 'scale']}, scoring=['roc_auc'],\n",
    "                        cv=skfold, refit='roc_auc', verbose=0, return_train_score=True)\n",
    "svm_nonlinear_2f.fit(X_train_2f, y_train_ohv)\n",
    "\n",
    "best_svm_nonlin_2f = svm_nonlinear_2f.best_estimator_\n",
    "print('Parameters to get best svm are:', svm_nonlinear_2f.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict + fit for LR and SVM (PCA):\n",
    "\n",
    "lr_best_model_2f.fit(X_train_2f, y_train)\n",
    "y_pred_test_2f = lr_best_model_2f.predict(X_test_2f)\n",
    "\n",
    "model_lr_2f = lr_best_model_2f\n",
    "model_svm_2f = best_svm_nonlin_2f\n",
    "\n",
    "y_pred_lr_2f = lr_best_model_2f.decision_function(X_test_2f)\n",
    "lr_fpr_2f, lr_tpr_2f, threshold_2f = roc_curve(y_test_ohv, y_pred_lr_2f)\n",
    "y_pred_lr_train_2f = (lr_best_model_2f.predict(X_train_2f)).astype(\"int32\")\n",
    "y_pred_lr_test_2f = (model_lr_2f.predict(X_test_2f)).astype(\"int32\")\n",
    "auc_lr_2f = metrics.roc_auc_score(y_test,y_pred_lr_test_2f)\n",
    "\n",
    "\n",
    "y_pred_svm_2f = svm_nonlinear_2f.decision_function(X_test_2f)\n",
    "svm_fpr_2f, svm_tpr_2f, threshold_2f = roc_curve(y_test_ohv, y_pred_svm_2f)\n",
    "y_pred_svm_train_2f = (svm_nonlinear_2f.predict(X_train_2f)).astype(\"int32\")\n",
    "y_pred_svm_test_2f = (svm_nonlinear_2f.predict(X_test_2f)).astype(\"int32\")\n",
    "auc_svm_2f = metrics.roc_auc_score(y_test,y_pred_svm_test_2f)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (5,5), dpi = 100)\n",
    "plt.plot(svm_fpr_2f, svm_tpr_2f, linestyle = '-', label = 'SVM - 2D (auc=%0.3f)' %auc_svm_2f)\n",
    "plt.plot(lr_fpr_2f, lr_tpr_2f,marker='.' ,label = 'Logistic Reg - 2D(auc=%0.3f)' %auc_lr_2f)\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Logistic Regression Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_lr_test_2f,y_pred_lr_train_2f)\n",
    "\n",
    "print()\n",
    "print('SVM Results:')\n",
    "parameter_evaluation(y_test_ohv,y_train_ohv,y_pred_svm_test_2f,y_pred_svm_train_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
